{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5822ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google import genai\n",
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5f54689",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATH = \"indexes/reference.index\"\n",
    "META_PATH = \"indexes/chunks_metadata.pkl\"\n",
    "CONFIG_PATH = \"indexes/index_config.json\"\n",
    "BM25_PATH = \"indexes/bm25.pkl\"\n",
    "\n",
    "with open(BM25_PATH, \"rb\") as f:\n",
    "    bm25 = pickle.load(f)\n",
    "\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "with open(META_PATH, \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "embedder = SentenceTransformer(config[\"embedding_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7eaf2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"LLM_HW_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key not set\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "model = client.models\n",
    "model_name = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "20db19b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 documents from corpus\n"
     ]
    }
   ],
   "source": [
    "with open(\"indexes/documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "print(\"Loaded\", len(documents), \"documents from corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "54c4e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_embedding(\n",
    "    code_snippet: str,\n",
    "    top_k: int = 5,\n",
    "    plagiarism_threshold: float = 0.42\n",
    "):\n",
    "\n",
    "    query_embedding = embedder.encode(\n",
    "        [code_snippet],\n",
    "        normalize_embeddings=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    max_sim = float(np.max(D[0]))\n",
    "\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        meta = chunked_docs[idx]\n",
    "\n",
    "        results.append({\n",
    "            \"similarity\": float(score),\n",
    "            \"file_name\": meta[\"file_name\"],\n",
    "            \"chunk_type\": meta[\"chunk_type\"],\n",
    "            \"matched_text_preview\": meta[\"text\"][:300]\n",
    "        })\n",
    "\n",
    "    plagiarism = max_sim >= plagiarism_threshold\n",
    "\n",
    "\n",
    "    if max_sim >= plagiarism_threshold:\n",
    "        verdict = 1\n",
    "    else:\n",
    "        verdict = 0\n",
    "\n",
    "    return {\n",
    "        \"plagiarism\": plagiarism,\n",
    "        \"max_similarity\": round(max_sim, 4),\n",
    "        \"verdict\": verdict,\n",
    "        \"matches\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f8842104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_llm(student_code: str):\n",
    "    \"\"\"\n",
    "    Direct LLM Analysis: Dumps the whole repo + student code into the prompt.\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus_texts = []\n",
    "    for doc in documents:\n",
    "        fname = doc.get('file_name', 'unknown_file')\n",
    "        content = doc.get('raw_code', doc.get('text', '')) \n",
    "        corpus_texts.append(f\"--- START FILE: {fname} ---\\n{content}\\n--- END FILE ---\\n\")\n",
    "    \n",
    "    full_repo_context = \"\\n\".join(corpus_texts)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a code plagiarism detection system.\n",
    "    \n",
    "    REFERENCE REPOSITORY (The Source of Truth):\n",
    "    {full_repo_context}\n",
    "    \n",
    "    SUSPICIOUS CODE (Student Submission):\n",
    "    {student_code}\n",
    "    \n",
    "    TASK:\n",
    "    Analyze the Suspicious Code. determine if it is plagiarized from the Reference Repository.\n",
    "    - Plagiarism includes: direct copying, variable renaming, removing comments, or restructuring while keeping logic.\n",
    "    - It is NOT plagiarism if the code uses a completely different algorithm.\n",
    "    \n",
    "    OUTPUT FORMAT:\n",
    "    Return valid JSON only. No markdown formatting.\n",
    "    {{\n",
    "        \"is_plagiarized\": true/false,\n",
    "        \"confidence_score\": 0.0 to 1.0,\n",
    "        \"explanation\": \"Brief reason citing specific file names from reference if found.\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt,\n",
    "            config={\"response_mime_type\": \"application/json\"}\n",
    "        )\n",
    "        \n",
    "        result_text = response.text\n",
    "        data = json.loads(result_text)\n",
    "        \n",
    "        return {\n",
    "            \"verdict\": 1 if data[\"is_plagiarized\"] else 0,\n",
    "            \"explanation\": data[\"explanation\"],\n",
    "            \"score\": data.get(\"confidence_score\", 0.0)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Error: {e}\")\n",
    "        return {\n",
    "            \"verdict\": \"Error\",\n",
    "            \"explanation\": str(e),\n",
    "            \"score\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db7e017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_rag(\n",
    "    student_code: str,\n",
    "    top_k: int = 5,\n",
    "    max_output_tokens: int = 1024\n",
    "):\n",
    "    query_emb = embedder.encode([student_code], normalize_embeddings=True).astype(\"float32\")\n",
    "    \n",
    "    D, I = index.search(query_emb, top_k)\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    context_texts = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        meta = chunked_docs[idx]\n",
    "        retrieved_chunks.append({\n",
    "            \"similarity\": float(score),\n",
    "            \"file_name\": meta[\"file_name\"],\n",
    "            \"chunk_type\": meta[\"chunk_type\"],\n",
    "            \"matched_text\": meta.get(\"original_code\", meta[\"text\"])  # use original code for LLM\n",
    "        })\n",
    "        context_texts.append(f\"File: {meta['file_name']}\\n{meta.get('original_code', meta['text'])}\\n\")\n",
    "    \n",
    "    context = \"\\n\".join(context_texts)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a code plagiarism detection assistant.\n",
    "You are given the following relevant reference code snippets:\n",
    "\n",
    "{context}\n",
    "\n",
    "Determine if the following student's code is plagiarized:\n",
    "\n",
    "{student_code}\n",
    "\n",
    "Instructions:\n",
    "- Provide a short verdict: \"Plagiarized\", \"Partially Plagiarized\", or \"Original\".\n",
    "- Explain briefly why.\n",
    "- Do not hallucinate file names.\n",
    "- Return in JSON format like:\n",
    "{{\"verdict\": \"...\", \"explanation\": \"...\"}}\n",
    "\"\"\"\n",
    "    \n",
    "    response = model.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt,\n",
    "            config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    content = response.last.split(\"\\n\")[0] if hasattr(response, \"last\") else response.text\n",
    "    \n",
    "    try:\n",
    "        result = json.loads(content)\n",
    "    except:\n",
    "        result = {\"verdict\": \"Unknown\", \"explanation\": content}\n",
    "    \n",
    "    result[\"plagiarism\"] = 1 if result.get(\"verdict\", \"\").lower() != \"original\" else 0\n",
    "    result[\"retrieved_chunks\"] = retrieved_chunks\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "db0a177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hybrid_rag(\n",
    "    student_code: str,\n",
    "    top_k: int = 5,\n",
    "    bm25_weight: float = 0.5,\n",
    "    embedding_weight: float = 0.5\n",
    "):\n",
    "    query_emb = embedder.encode([student_code], normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = index.search(query_emb, top_k*3)\n",
    "    \n",
    "    embedding_candidates = {idx: float(score) for score, idx in zip(D[0], I[0])}\n",
    "    \n",
    "    tokenized_query = re.findall(r\"\\w+\", student_code)\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    fused_scores = {}\n",
    "    for idx in range(len(chunked_docs)):\n",
    "        emb_score = embedding_candidates.get(idx, 0)\n",
    "        bm25_score = bm25_scores[idx] if idx < len(bm25_scores) else 0\n",
    "        fused_scores[idx] = embedding_weight*emb_score + bm25_weight*bm25_score\n",
    "    \n",
    "    top_indices = sorted(fused_scores, key=fused_scores.get, reverse=True)[:top_k]\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    context_texts = []\n",
    "    for idx in top_indices:\n",
    "        meta = chunked_docs[idx]\n",
    "        score = fused_scores[idx]\n",
    "        retrieved_chunks.append({\n",
    "            \"file_name\": meta[\"file_name\"],\n",
    "            \"chunk_type\": meta[\"chunk_type\"],\n",
    "            \"similarity\": float(score),\n",
    "            \"matched_text\": meta.get(\"original_code\", meta[\"text\"])\n",
    "        })\n",
    "        context_texts.append(f\"File: {meta['file_name']}\\n{meta.get('original_code', meta['text'])}\\n\")\n",
    "    \n",
    "\n",
    "    context = \"\\n\".join(context_texts)\n",
    "    prompt = f\"\"\"\n",
    "You are a code plagiarism detection assistant.\n",
    "You are given the following top candidate reference code snippets:\n",
    "\n",
    "{context}\n",
    "\n",
    "Determine if the following student's code is plagiarized:\n",
    "\n",
    "{student_code}\n",
    "\n",
    "Instructions:\n",
    "- Provide a short verdict: \"Plagiarized\", \"Partially Plagiarized\", or \"Original\".\n",
    "- Explain briefly why.\n",
    "- Do not hallucinate file names.\n",
    "- Return in JSON format like:\n",
    "{{\"verdict\": \"...\", \"explanation\": \"...\"}}\n",
    "\"\"\"\n",
    "    \n",
    "    response = model.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt,\n",
    "            config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    content = response.last.split(\"\\n\")[0] if hasattr(response, \"last\") else response.text\n",
    "    import json\n",
    "    try:\n",
    "        result = json.loads(content)\n",
    "    except:\n",
    "        result = {\"verdict\": \"Unknown\", \"explanation\": content}\n",
    "    \n",
    "    result[\"plagiarism\"] = 1 if result.get(\"verdict\", \"\").lower() != \"original\" else 0\n",
    "    result[\"retrieved_chunks\"] = retrieved_chunks\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
